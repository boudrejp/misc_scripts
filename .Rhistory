cars.mpg$carname <- NULL
lapply(cars.mpg, class)
###take out mpg for predictors
predictors <- colnames(cars.mpg)[-1]
###separate into train/test
intrain <- createDataPartition(cars.mpg$mpg, times = 1, p = 0.65)$Resample1
intest <- c(1:nrow(cars.mpg))[-intrain]
traindata <- cars.mpg[intrain,]
testdata <- cars.mpg[intest,]
model <- cubist(x = traindata[,predictors], y = traindata$mpg, committees = 5)
##make predictions on test data
testdata$pred <- predict(model, newdata = testdata[,predictors], neighbors = 1)
###plort results
gg <- ggplot(testdata) + geom_point(aes(x=c(1:dim(testdata)[1]), y = mpg), col = "purple") + geom_line(aes(x=c(1:dim(testdata)[1]), y = pred), col = "red")
plot(gg)
summaryu(model)
summary(model)
cars.mpg
summary(cars.mpg)
model$vars$all
model$vars$used
###cubist demo for trying out new function
###use cars mpg dataset: https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/
set.seed(50)
require(Cubist)
require(caret)
cars.mpg <- read.csv("C:/Users/johnb/Documents/Code Projects/Github/carsmpg.csv", stringsAsFactors = FALSE)
cars.mpg$carname <- gsub("\"", "", cars.mpg$carname)
head(cars.mpg)
###PREPROCESSING: make sure data is in correct data types for model
###there's certain parameters that we'd like to have as factors, rather than numeric
###in addition, there's a few missing values listed as "?" under cars.mpg$horsepower.
rows.to.delete <- which(cars.mpg$horsepower == "?")
tot.rows <- nrow(cars.mpg)
print(paste("We are looking to get rid of", length(rows.to.delete), "rows of", tot.rows, "total rows."))
###6 << 398. we can probably get rid of these without significantly effecting the analysis
cars.mpg <- cars.mpg[-rows.to.delete,]
###lapply gives the results we want in order to check the classes
lapply(cars.mpg, class)
###at this point, it makes sense to have horsepower as a numeric, cylinders as a factor, and origin as a factor
cars.mpg[["cylinders"]] <- as.factor(cars.mpg[["cylinders"]])
cars.mpg[["origin"]] <- as.factor(cars.mpg[["origin"]])
cars.mpg[["horsepower"]] <- as.numeric(cars.mpg[["horsepower"]])
###car name doesn't really make sense to have as a predictor, unless we break the strings into discrete manufacturers
###given that this is an example, I just won't bother with it
cars.mpg$carname <- NULL
lapply(cars.mpg, class)
###take out mpg for predictors
predictors <- colnames(cars.mpg)[-1]
###separate into train/test
intrain <- createDataPartition(cars.mpg$mpg, times = 1, p = 0.65)$Resample1
intest <- c(1:nrow(cars.mpg))[-intrain]
traindata <- cars.mpg[intrain,]
testdata <- cars.mpg[intest,]
model <- cubist(x = traindata[,predictors], y = traindata$mpg, committees = 3)
##make predictions on test data
testdata$pred <- predict(model, newdata = testdata[,predictors], neighbors = 1)
###plort results
gg <- ggplot(testdata) + geom_point(aes(x=c(1:dim(testdata)[1]), y = mpg), col = "purple") + geom_line(aes(x=c(1:dim(testdata)[1]), y = pred), col = "red")
plot(gg)
###cubist demo for trying out new function
###use cars mpg dataset: https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/
set.seed(50)
require(Cubist)
require(caret)
cars.mpg <- read.csv("C:/Users/johnb/Documents/Code Projects/Github/carsmpg.csv", stringsAsFactors = FALSE)
cars.mpg$carname <- gsub("\"", "", cars.mpg$carname)
head(cars.mpg)
###PREPROCESSING: make sure data is in correct data types for model
###there's certain parameters that we'd like to have as factors, rather than numeric
###in addition, there's a few missing values listed as "?" under cars.mpg$horsepower.
rows.to.delete <- which(cars.mpg$horsepower == "?")
tot.rows <- nrow(cars.mpg)
print(paste("We are looking to get rid of", length(rows.to.delete), "rows of", tot.rows, "total rows."))
###6 << 398. we can probably get rid of these without significantly effecting the analysis
cars.mpg <- cars.mpg[-rows.to.delete,]
###lapply gives the results we want in order to check the classes
lapply(cars.mpg, class)
###at this point, it makes sense to have horsepower as a numeric, cylinders as a factor, and origin as a factor
cars.mpg[["cylinders"]] <- as.factor(cars.mpg[["cylinders"]])
cars.mpg[["origin"]] <- as.factor(cars.mpg[["origin"]])
cars.mpg[["horsepower"]] <- as.numeric(cars.mpg[["horsepower"]])
###car name doesn't really make sense to have as a predictor, unless we break the strings into discrete manufacturers
###given that this is an example, I just won't bother with it
cars.mpg$carname <- NULL
lapply(cars.mpg, class)
###take out mpg for predictors
predictors <- colnames(cars.mpg)[-1]
###separate into train/test
intrain <- createDataPartition(cars.mpg$mpg, times = 1, p = 0.65)$Resample1
intest <- c(1:nrow(cars.mpg))[-intrain]
traindata <- cars.mpg[intrain,]
testdata <- cars.mpg[intest,]
model <- cubist(x = traindata[,predictors], y = traindata$mpg, committees = 3)
##make predictions on test data
testdata$pred <- predict(model, newdata = testdata[,predictors], neighbors = 1)
###plort results
gg <- ggplot(testdata) + geom_point(aes(x=c(1:dim(testdata)[1]), y = mpg), col = "purple") + geom_line(aes(x=c(1:dim(testdata)[1]), y = pred), col = "red")
plot(gg)
summary(model)
###cubist demo for trying out new function
###use cars mpg dataset: https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/
set.seed(50)
require(Cubist)
require(caret)
cars.mpg <- read.csv("C:/Users/johnb/Documents/Code Projects/Github/carsmpg.csv", stringsAsFactors = FALSE)
cars.mpg$carname <- gsub("\"", "", cars.mpg$carname)
head(cars.mpg)
###PREPROCESSING: make sure data is in correct data types for model
###there's certain parameters that we'd like to have as factors, rather than numeric
###in addition, there's a few missing values listed as "?" under cars.mpg$horsepower.
rows.to.delete <- which(cars.mpg$horsepower == "?")
tot.rows <- nrow(cars.mpg)
print(paste("We are looking to get rid of", length(rows.to.delete), "rows of", tot.rows, "total rows."))
###6 << 398. we can probably get rid of these without significantly effecting the analysis
cars.mpg <- cars.mpg[-rows.to.delete,]
###lapply gives the results we want in order to check the classes
lapply(cars.mpg, class)
###at this point, it makes sense to have horsepower as a numeric, cylinders as a factor, and origin as a factor
cars.mpg[["cylinders"]] <- as.factor(cars.mpg[["cylinders"]])
cars.mpg[["origin"]] <- as.factor(cars.mpg[["origin"]])
cars.mpg[["horsepower"]] <- as.numeric(cars.mpg[["horsepower"]])
###car name doesn't really make sense to have as a predictor, unless we break the strings into discrete manufacturers
###given that this is an example, I just won't bother with it
cars.mpg$carname <- NULL
lapply(cars.mpg, class)
###take out mpg for predictors
predictors <- colnames(cars.mpg)[-1]
###separate into train/test
intrain <- createDataPartition(cars.mpg$mpg, times = 1, p = 0.65)$Resample1
intest <- c(1:nrow(cars.mpg))[-intrain]
traindata <- cars.mpg[intrain,]
testdata <- cars.mpg[intest,]
model <- cubist(x = traindata[,predictors], y = traindata$mpg, committees = 10)
##make predictions on test data
testdata$pred <- predict(model, newdata = testdata[,predictors], neighbors = 1)
###plort results
gg <- ggplot(testdata) + geom_point(aes(x=c(1:dim(testdata)[1]), y = mpg), col = "purple") + geom_line(aes(x=c(1:dim(testdata)[1]), y = pred), col = "red")
plot(gg)
summary(model)
###cubist demo for trying out new function
###use cars mpg dataset: https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/
set.seed(50)
require(Cubist)
require(caret)
cars.mpg <- read.csv("C:/Users/johnb/Documents/Code Projects/Github/carsmpg.csv", stringsAsFactors = FALSE)
cars.mpg$carname <- gsub("\"", "", cars.mpg$carname)
head(cars.mpg)
###PREPROCESSING: make sure data is in correct data types for model
###there's certain parameters that we'd like to have as factors, rather than numeric
###in addition, there's a few missing values listed as "?" under cars.mpg$horsepower.
rows.to.delete <- which(cars.mpg$horsepower == "?")
tot.rows <- nrow(cars.mpg)
print(paste("We are looking to get rid of", length(rows.to.delete), "rows of", tot.rows, "total rows."))
###6 << 398. we can probably get rid of these without significantly effecting the analysis
cars.mpg <- cars.mpg[-rows.to.delete,]
###lapply gives the results we want in order to check the classes
lapply(cars.mpg, class)
###at this point, it makes sense to have horsepower as a numeric, cylinders as a factor, and origin as a factor
cars.mpg[["cylinders"]] <- as.factor(cars.mpg[["cylinders"]])
cars.mpg[["origin"]] <- as.factor(cars.mpg[["origin"]])
cars.mpg[["horsepower"]] <- as.numeric(cars.mpg[["horsepower"]])
###car name doesn't really make sense to have as a predictor, unless we break the strings into discrete manufacturers
###given that this is an example, I just won't bother with it
cars.mpg$carname <- NULL
lapply(cars.mpg, class)
###take out mpg for predictors
predictors <- colnames(cars.mpg)[-1]
###separate into train/test
intrain <- createDataPartition(cars.mpg$mpg, times = 1, p = 0.65)$Resample1
intest <- c(1:nrow(cars.mpg))[-intrain]
traindata <- cars.mpg[intrain,]
testdata <- cars.mpg[intest,]
model <- cubist(x = traindata[,predictors], y = traindata$mpg, committees = 15)
##make predictions on test data
testdata$pred <- predict(model, newdata = testdata[,predictors], neighbors = 1)
###plort results
gg <- ggplot(testdata) + geom_point(aes(x=c(1:dim(testdata)[1]), y = mpg), col = "purple") + geom_line(aes(x=c(1:dim(testdata)[1]), y = pred), col = "red")
plot(gg)
summary(model)
###cubist demo for trying out new function
###use cars mpg dataset: https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/
set.seed(50)
require(Cubist)
require(caret)
cars.mpg <- read.csv("C:/Users/johnb/Documents/Code Projects/Github/carsmpg.csv", stringsAsFactors = FALSE)
cars.mpg$carname <- gsub("\"", "", cars.mpg$carname)
head(cars.mpg)
###PREPROCESSING: make sure data is in correct data types for model
###there's certain parameters that we'd like to have as factors, rather than numeric
###in addition, there's a few missing values listed as "?" under cars.mpg$horsepower.
rows.to.delete <- which(cars.mpg$horsepower == "?")
tot.rows <- nrow(cars.mpg)
print(paste("We are looking to get rid of", length(rows.to.delete), "rows of", tot.rows, "total rows."))
###6 << 398. we can probably get rid of these without significantly effecting the analysis
cars.mpg <- cars.mpg[-rows.to.delete,]
###lapply gives the results we want in order to check the classes
lapply(cars.mpg, class)
###at this point, it makes sense to have horsepower as a numeric, cylinders as a factor, and origin as a factor
cars.mpg[["cylinders"]] <- as.factor(cars.mpg[["cylinders"]])
cars.mpg[["origin"]] <- as.factor(cars.mpg[["origin"]])
cars.mpg[["horsepower"]] <- as.numeric(cars.mpg[["horsepower"]])
###car name doesn't really make sense to have as a predictor, unless we break the strings into discrete manufacturers
###given that this is an example, I just won't bother with it
cars.mpg$carname <- NULL
lapply(cars.mpg, class)
###take out mpg for predictors
predictors <- colnames(cars.mpg)[-1]
###separate into train/test
intrain <- createDataPartition(cars.mpg$mpg, times = 1, p = 0.65)$Resample1
intest <- c(1:nrow(cars.mpg))[-intrain]
traindata <- cars.mpg[intrain,]
testdata <- cars.mpg[intest,]
model <- cubist(x = traindata[,predictors], y = traindata$mpg, committees =30)
##make predictions on test data
testdata$pred <- predict(model, newdata = testdata[,predictors], neighbors = 1)
###plort results
gg <- ggplot(testdata) + geom_point(aes(x=c(1:dim(testdata)[1]), y = mpg), col = "purple") + geom_line(aes(x=c(1:dim(testdata)[1]), y = pred), col = "red")
plot(gg)
summary(model)
model <- cubist(x = traindata[,predictors], y = traindata$mpg, committees =10)
##make predictions on test data
testdata$pred <- predict(model, newdata = testdata[,predictors], neighbors = 1)
###plort results
gg <- ggplot(testdata) + geom_point(aes(x=c(1:dim(testdata)[1]), y = mpg), col = "purple") + geom_line(aes(x=c(1:dim(testdata)[1]), y = pred), col = "red")
plot(gg)
summary(model)
lapply(cars.mpg, class)
###lapply gives the results we want in order to check the classes
lapply(cars.mpg, class)
summary(model)
cubist.model <- modell
cubist.model <- model
prediction.data <- traindata
require(dprep)
require(Cubist)
###step 0: basic error catching
if(class(cubist.model) != "cubist"){
stop("Must input cubist model")
}
if(class(prediction.data) != "data.frame"){
stop("input data must be data frame")
}
if(colnames(prediction.data) != cubist.model$vars$all){
stop("prediction data names do not match cubist parameters")
}
###step 1: reconstruct training data frame
data.strings <- strsplit(cubist.model$data, "\\n")
all.data <- lapply(data.strings, function(x){strsplit(x, ",")})
all.data <- t(as.data.frame(all.data))
rownames(all.data) <- NULL
cubist.model$vars$all
cubist.model$vars$used
colnames(prediction.data)
###take out mpg for predictors
predictors <- colnames(cars.mpg)[-1]
cubist.model$names
colnames(prediction.data)
cubist.model$vars$all
cubist.model$output
cubist.model$usage
cubist.model$vars
colnames(prediction.data)
if(predictors != cubist.model$vars$all){
stop("prediction data names do not match cubist parameters")
}
predictors
cubist.model$vars$all
predictors != cubist.model$vars$all
if(predictors == cubist.model$vars$all){
stop("prediction data names do not match cubist parameters")
}
predictors == cubist.model$vars$all
if(all(predictors == cubist.model$vars$all)){
stop("prediction data names do not match cubist parameters")
}
if(!all(predictors == cubist.model$vars$all)){
stop("prediction data names do not match cubist parameters")
}
###step 1: reconstruct training data frame
data.strings <- strsplit(cubist.model$data, "\\n")
all.data <- lapply(data.strings, function(x){strsplit(x, ",")})
all.data <- t(as.data.frame(all.data))
rownames(all.data) <- NULL
###the first column is the y value. we should drop it, not useful for what we're doing here
all.data <- all.data[,c(2:ncol(all.data))]
colnames(all.data) <- predictors
all.data
lapply(all.data, class)
typeof(all.data)
all.data
###step 1: reconstruct training data frame
data.strings <- strsplit(cubist.model$data, "\\n")
all.data <- lapply(data.strings, function(x){strsplit(x, ",")})
all.data <- t(as.data.frame(all.data))
rownames(all.data) <- NULL
all.data
summaru(all.data)
summary(all.data)
###the first column is the y value. we should drop it, not useful for what we're doing here
all.data <- all.data[,c(2:ncol(all.data))]
colnames(all.data) <- predictors
summary(all.data)
cubist.model <- model
prediction.data <- traindata
predictors <- predictors
if(class(cubist.model) != "cubist"){
stop("Must input cubist model")
}
if(class(prediction.data) != "data.frame"){
stop("input data must be data frame")
}
if(!all(predictors == cubist.model$vars$all)){
stop("prediction data names do not match cubist parameters")
}
###step 1: reconstruct training data frame
data.strings <- strsplit(cubist.model$data, "\\n")
all.data <- lapply(data.strings, function(x){strsplit(x, ",")})
all.data <- t(as.data.frame(all.data))
rownames(all.data) <- NULL
###the first column is the y value. we should drop it, not useful for what we're doing here
all.data <- all.data[,c(2:ncol(all.data))]
colnames(all.data) <- predictors
###the first column is the y value. we should drop it, not useful for what we're doing here
all.data <- as.data.frame(all.data[,c(2:ncol(all.data))])
colnames(all.data) <- predictors
###step 1: reconstruct training data frame
data.strings <- strsplit(cubist.model$data, "\\n")
all.data <- lapply(data.strings, function(x){strsplit(x, ",")})
all.data <- t(as.data.frame(all.data))
rownames(all.data) <- NULL
###the first column is the y value. we should drop it, not useful for what we're doing here
all.data <- as.data.frame(all.data[,c(2:ncol(all.data))])
colnames(all.data) <- predictors
summmary(all.data)
summary(all.data)
apply(all.data, 2, class)
lapply(all.data, class)
lapply(prediction.data, class)
length(prediction.data)
length(all.data)
length(prediction.data[[predictrors]])
length(prediction.data[[predictors]])
training.data[[predictors]]
prediction.data[[predictors]]
levels(all.data)
colnames(all.data)
all.data[[[colnames(all.data)[1]]]]
all.data[[colnames(all.data)[1]]]]
all.data[[colnames(all.data)[1]]]
###we have now reconstructed the training data in all.data df, let's duplicate the class of the triaining data
for(name in predictors){
class(all.data[[name]]) <- class(prediction.data[[name]])
}
lapply(all.data, clas)
lapply(all.data, class)
###step 0: basic error catching
if(class(cubist.model) != "cubist"){
stop("Must input cubist model")
}
if(class(prediction.data) != "data.frame"){
stop("input data must be data frame")
}
if(!all(predictors == cubist.model$vars$all)){
stop("prediction data names do not match cubist parameters")
}
###step 1: reconstruct training data frame
data.strings <- strsplit(cubist.model$data, "\\n")
all.data <- lapply(data.strings, function(x){strsplit(x, ",")})
all.data <- t(as.data.frame(all.data))
rownames(all.data) <- NULL
###the first column is the y value. we should drop it, not useful for what we're doing here
all.data <- as.data.frame(all.data[,c(2:ncol(all.data))])
colnames(all.data) <- predictors
###we have now reconstructed the training data in all.data df, let's duplicate the class of the triaining data
for(name in predictors){
class(all.data[[name]]) <- class(prediction.data[[name]])
}
###now we move on to classifying the predictions as type 1, 2, or 3
###type 1: data is outside of MIN/MAX of training data-> data is an extrapolation. Lower confidence
###type 2: data is an outlier from multivariate perspective. look at something like knn distance
###type 3: data is well within scope of training data
out.vect <- rep(3, nrow(prediction.data))
###detect type 1 outliers
mins <- apply(all.data, 2, min)
maxs <- apply(all.data, 2, max)
###TODO: rewrite with apply statements
for(i in 1:nrow(prediction.data)){
for(j in 1:ncol(prediction.data)){
###first, check if column is factor or numeric
if(class(prediction.data[i,j]) == "factor"){
###make sure that the i'th entry is a factor included in training set
if(!(prediction.data[i,j] %in% unique(all.data[[j]]))){
out.vect[i] <- "class"
break
}
}
if(class(prediction.data[i,j]) == "numeric"){
###make sure that the i'th entry is within range of training set
if(prediction.data[i,j] > mins[j] | prediction.data[i,j] < maxs[j]){
out.vect[i] <- "number"
break
}
}
}
out.vext
out.vect
.
$
out.vect
###cubist demo for trying out new function
###use cars mpg dataset: https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/
set.seed(50)
require(Cubist)
require(caret)
cars.mpg <- read.csv("C:/Users/johnb/Documents/Code Projects/Github/carsmpg.csv", stringsAsFactors = FALSE)
cars.mpg$carname <- gsub("\"", "", cars.mpg$carname)
head(cars.mpg)
###PREPROCESSING: make sure data is in correct data types for model
###there's certain parameters that we'd like to have as factors, rather than numeric
###in addition, there's a few missing values listed as "?" under cars.mpg$horsepower.
rows.to.delete <- which(cars.mpg$horsepower == "?")
tot.rows <- nrow(cars.mpg)
print(paste("We are looking to get rid of", length(rows.to.delete), "rows of", tot.rows, "total rows."))
###6 << 398. we can probably get rid of these without significantly effecting the analysis
cars.mpg <- cars.mpg[-rows.to.delete,]
###lapply gives the results we want in order to check the classes
lapply(cars.mpg, class)
###at this point, it makes sense to have horsepower as a numeric, cylinders as a factor, and origin as a factor
cars.mpg[["cylinders"]] <- as.factor(cars.mpg[["cylinders"]])
cars.mpg[["origin"]] <- as.factor(cars.mpg[["origin"]])
cars.mpg[["horsepower"]] <- as.numeric(cars.mpg[["horsepower"]])
###car name doesn't really make sense to have as a predictor, unless we break the strings into discrete manufacturers
###given that this is an example, I just won't bother with it
cars.mpg$carname <- NULL
lapply(cars.mpg, class)
###take out mpg for predictors
predictors <- colnames(cars.mpg)[-1]
###separate into train/test
intrain <- createDataPartition(cars.mpg$mpg, times = 1, p = 0.5)$Resample1
intest <- c(1:nrow(cars.mpg))[-intrain]
traindata <- cars.mpg[intrain,]
testdata <- cars.mpg[intest,]
model <- cubist(x = traindata[,predictors], y = traindata$mpg, committees =10)
##make predictions on test data
testdata$pred <- predict(model, newdata = testdata[,predictors], neighbors = 1)
###plort results
gg <- ggplot(testdata) + geom_point(aes(x=c(1:dim(testdata)[1]), y = mpg), col = "purple") + geom_line(aes(x=c(1:dim(testdata)[1]), y = pred), col = "red")
plot(gg)
summary(model)
cubist.model <- model
prediction.data <- traindata
predictors
######################################
### author: John Boudreaux ###########
### date: 2018-01-12 #################
######################################
require(Cubist)
require(dprep)
find.cubist.outliers <- function(cubist.model, prediction.data, predictors){
###step 0: basic error catching
if(class(cubist.model) != "cubist"){
stop("Must input cubist model")
}
if(class(prediction.data) != "data.frame"){
stop("input data must be data frame")
}
if(!all(predictors == cubist.model$vars$all)){
stop("prediction data names do not match cubist parameters")
}
###step 1: reconstruct training data frame
data.strings <- strsplit(cubist.model$data, "\\n")
all.data <- lapply(data.strings, function(x){strsplit(x, ",")})
all.data <- t(as.data.frame(all.data))
rownames(all.data) <- NULL
###the first column is the y value. we should drop it, not useful for what we're doing here
all.data <- as.data.frame(all.data[,c(2:ncol(all.data))])
colnames(all.data) <- predictors
###we have now reconstructed the training data in all.data df, let's duplicate the class of the triaining data
for(name in predictors){
class(all.data[[name]]) <- class(prediction.data[[name]])
}
###now we move on to classifying the predictions as type 1, 2, or 3
###type 1: data is outside of MIN/MAX of training data-> data is an extrapolation. Lower confidence
###type 2: data is an outlier from multivariate perspective. look at something like knn distance
###type 3: data is well within scope of training data
out.vect <- rep(3, nrow(prediction.data))
###detect type 1 outliers
mins <- apply(all.data, 2, min)
maxs <- apply(all.data, 2, max)
###TODO: rewrite with apply statements
for(i in 1:nrow(prediction.data)){
for(j in 1:ncol(prediction.data)){
###first, check if column is factor or numeric
if(class(prediction.data[i,j]) == "factor"){
###make sure that the i'th entry is a factor included in training set
if(!(prediction.data[i,j] %in% unique(all.data[[j]]))){
out.vect[i] <- 2
break
}
}
if(class(prediction.data[i,j]) == "numeric"){
###make sure that the i'th entry is within range of training set
if(prediction.data[i,j] > mins[j] | prediction.data[i,j] < maxs[j]){
out.vect[i] <- 2
break
}
}
}
}
return(out.vext)}
out.vect
